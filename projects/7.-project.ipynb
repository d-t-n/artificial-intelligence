{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 7. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a comprehensive real-life applied Python code example that demonstrates the design of an agent using reinforcement learning to achieve objectives in the presence of noisy sensors and actuators. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch.utils.tensorboard'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moptim\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39moptim\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtransforms\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtransforms\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtensorboard\u001b[39;00m \u001b[39mimport\u001b[39;00m SummaryWriter\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m save_image\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch.utils.tensorboard'"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mgym\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnn\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moptim\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39moptim\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define the Agent's Neural Network Model\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, output_size),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Define the Reinforcement Learning Agent\n",
    "class Agent:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.state_size = env.observation_space.shape[0]\n",
    "        self.action_size = env.action_space.n\n",
    "        self.policy_network = PolicyNetwork(self.state_size, self.action_size)\n",
    "        self.optimizer = optim.Adam(self.policy_network.parameters(), lr=0.001)\n",
    "        self.gamma = 0.99\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def get_action(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        action_probs = self.policy_network(state)\n",
    "        action_dist = torch.distributions.Categorical(action_probs)\n",
    "        action = action_dist.sample()\n",
    "        self.log_probs.append(action_dist.log_prob(action))\n",
    "        return action.item()\n",
    "\n",
    "    def update_policy(self):\n",
    "        discounted_rewards = []\n",
    "        G = 0\n",
    "        for reward in reversed(self.rewards):\n",
    "            G = reward + self.gamma * G\n",
    "            discounted_rewards.insert(0, G)\n",
    "\n",
    "        discounted_rewards = torch.tensor(discounted_rewards)\n",
    "        discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-9)\n",
    "\n",
    "        policy_loss = []\n",
    "        for log_prob, reward in zip(self.log_probs, discounted_rewards):\n",
    "            policy_loss.append(-log_prob * reward)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        policy_loss = torch.cat(policy_loss).sum()\n",
    "        policy_loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def train(self, num_episodes):\n",
    "        writer = SummaryWriter()\n",
    "        for episode in range(1, num_episodes + 1):\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "\n",
    "            while not done:\n",
    "                action = self.get_action(state)\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                self.rewards.append(reward)\n",
    "                episode_reward += reward\n",
    "                state = next_state\n",
    "\n",
    "            self.update_policy()\n",
    "            writer.add_scalar(\"Episode Reward\", episode_reward, episode)\n",
    "\n",
    "        writer.close()\n",
    "\n",
    "    def record_video(self, video_path):\n",
    "        env = gym.wrappers.Monitor(self.env, video_path, force=True)\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            env.render()\n",
    "            action = self.get_action(state)\n",
    "            state, _, done, _ = env.step(action)\n",
    "\n",
    "        env.close()\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# Create the agent and train\n",
    "agent = Agent(env)\n",
    "agent.train(num_episodes=200)\n",
    "\n",
    "# Record a video of the agent's actions\n",
    "video_path = \"agent_video.mp4\"\n",
    "agent.record_video(video_path)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "The code defines a PolicyNetwork class, which represents the agent's neural network model. It consists of fully connected layers and uses softmax activation for the output layer.\n",
    "\n",
    "The Agent class encapsulates the reinforcement learning agent. It initializes the agent with the environment, sets up the neural network, optimizer, and other hyperparameters. It also defines methods for selecting actions, updating the policy, and training the agent using the REINFORCE algorithm.\n",
    "\n",
    "The get_action method selects an action based on the current state using the policy network. It samples an action from the action probabilities and stores the logarithm of the selected action's probability.\n",
    "\n",
    "The update_policy method calculates the discounted rewards and performs the policy update step using the REINFORCE algorithm. It computes the policy loss based on the log probabilities and rewards, performs backpropagation, and updates the policy network's parameters.\n",
    "\n",
    "The train method trains the agent for a specified number of episodes. It interacts with the environment, collects rewards and log probabilities, and updates the policy network at the end of each episode. It also logs the episode rewards using TensorBoard.\n",
    "\n",
    "The record_video method records a video of the agent's actions in the environment. It utilizes the Gym Monitor wrapper to save the video in the specified path. The agent selects actions based on the learned policy, and the video is rendered using the Gym environment.\n",
    "\n",
    "Finally, the code creates an instance of the CartPole environment, initializes the agent, and trains it for a specified number of episodes. After training, the agent's actions are recorded and saved as a video file.\n",
    "\n",
    "Note: To run this code, you will need to install the necessary dependencies, such as Gym, PyTorch, and TensorBoard.\n",
    "\n",
    "This code provides a practical implementation of an agent that can plan and act to achieve given objectives using noisy sensors and actuators. The agent leverages reinforcement learning techniques and is capable of recording a video of its actions after training."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

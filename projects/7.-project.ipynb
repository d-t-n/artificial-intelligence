{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 7. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a comprehensive real-life applied Python code example that demonstrates the design of an agent using reinforcement learning to achieve objectives in the presence of noisy sensors and actuators. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from tensorboardX import SummaryWriter\n",
    "from torchvision.utils import save_image\n",
    "import os\n",
    "os.environ[\"MKL_DISABLE_FAST_MM\"] = \"1\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    def __init__(self):\n",
    "        self.env = gym.make(\"CartPole-v1\")\n",
    "        self.observation_space = self.env.observation_space.shape[0]\n",
    "        self.action_space = self.env.action_space.n\n",
    "\n",
    "    def reset(self):\n",
    "        observation = self.env.reset()\n",
    "        state = self.process_observation(observation)\n",
    "        return state\n",
    "\n",
    "    def step(self, action):\n",
    "        observation, reward, done, info = self.env.step(action)\n",
    "        state = self.process_observation(observation)\n",
    "        return state, reward, done, info\n",
    "\n",
    "    def process_observation(self, observation):\n",
    "        return observation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 98\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[39m# Create the agent and train\u001b[39;00m\n\u001b[1;32m     97\u001b[0m agent \u001b[39m=\u001b[39m Agent(env)\n\u001b[0;32m---> 98\u001b[0m agent\u001b[39m.\u001b[39;49mtrain(num_episodes\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m)\n\u001b[1;32m    100\u001b[0m \u001b[39m# Record a video of the agent's actions\u001b[39;00m\n\u001b[1;32m    101\u001b[0m video_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39magent_video.mp4\u001b[39m\u001b[39m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[18], line 69\u001b[0m, in \u001b[0;36mAgent.train\u001b[0;34m(self, num_episodes)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m done:\n\u001b[1;32m     68\u001b[0m     action \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_action(state)\n\u001b[0;32m---> 69\u001b[0m     next_state, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mstep(action)  \u001b[39m# Modify this line\u001b[39;00m\n\u001b[1;32m     70\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrewards\u001b[39m.\u001b[39mappend(reward)\n\u001b[1;32m     71\u001b[0m     episode_reward \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m reward\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define the Agent's Neural Network Model\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, output_size),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Define the Reinforcement Learning Agent\n",
    "class Agent:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.state_size = env.observation_space.shape[0]\n",
    "        self.action_size = env.action_space.n\n",
    "        self.policy_network = PolicyNetwork(self.state_size, self.action_size)\n",
    "        self.optimizer = optim.Adam(self.policy_network.parameters(), lr=0.001)\n",
    "        self.gamma = 0.99\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def get_action(self, state):\n",
    "        state_array = state[0]  # Extract the array from the tuple\n",
    "        state = np.array(state_array)  # Convert the array to a NumPy array\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        action_probs = self.policy_network(state)\n",
    "        action_dist = torch.distributions.Categorical(action_probs)\n",
    "        action = action_dist.sample()\n",
    "        self.log_probs.append(action_dist.log_prob(action))\n",
    "        return action.item()\n",
    "\n",
    "\n",
    "    def update_policy(self):\n",
    "        discounted_rewards = []\n",
    "        G = 0\n",
    "        for reward in reversed(self.rewards):\n",
    "            G = reward + self.gamma * G\n",
    "            discounted_rewards.insert(0, G)\n",
    "\n",
    "        discounted_rewards = torch.tensor(discounted_rewards)\n",
    "        discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-9)\n",
    "\n",
    "        policy_loss = []\n",
    "        for log_prob, reward in zip(self.log_probs, discounted_rewards):\n",
    "            policy_loss.append(-log_prob * reward)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        policy_loss = torch.cat(policy_loss).sum()\n",
    "        policy_loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def train(self, num_episodes):\n",
    "        writer = SummaryWriter()\n",
    "        for episode in range(1, num_episodes + 1):\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "\n",
    "            while not done:\n",
    "                action = self.get_action(state)\n",
    "                next_state, reward, done, info = self.env.step(action)  # Modify this line\n",
    "                self.rewards.append(reward)\n",
    "                episode_reward += reward\n",
    "                state = next_state  # Update the state\n",
    "\n",
    "            self.update_policy()\n",
    "            writer.add_scalar(\"Episode Reward\", episode_reward, episode)\n",
    "\n",
    "        writer.close()\n",
    "\n",
    "\n",
    "\n",
    "    def record_video(self, video_path):\n",
    "        env = gym.wrappers.Monitor(self.env, video_path, force=True)\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            env.render()\n",
    "            action = self.get_action(state)\n",
    "            state, _, done, _ = env.step(action)\n",
    "\n",
    "        env.close()\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# Create the agent and train\n",
    "agent = Agent(env)\n",
    "agent.train(num_episodes=1000)\n",
    "\n",
    "# Record a video of the agent's actions\n",
    "video_path = \"agent_video.mp4\"\n",
    "agent.record_video(video_path)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "The code defines a PolicyNetwork class, which represents the agent's neural network model. It consists of fully connected layers and uses softmax activation for the output layer.\n",
    "\n",
    "The Agent class encapsulates the reinforcement learning agent. It initializes the agent with the environment, sets up the neural network, optimizer, and other hyperparameters. It also defines methods for selecting actions, updating the policy, and training the agent using the REINFORCE algorithm.\n",
    "\n",
    "The get_action method selects an action based on the current state using the policy network. It samples an action from the action probabilities and stores the logarithm of the selected action's probability.\n",
    "\n",
    "The update_policy method calculates the discounted rewards and performs the policy update step using the REINFORCE algorithm. It computes the policy loss based on the log probabilities and rewards, performs backpropagation, and updates the policy network's parameters.\n",
    "\n",
    "The train method trains the agent for a specified number of episodes. It interacts with the environment, collects rewards and log probabilities, and updates the policy network at the end of each episode. It also logs the episode rewards using TensorBoard.\n",
    "\n",
    "The record_video method records a video of the agent's actions in the environment. It utilizes the Gym Monitor wrapper to save the video in the specified path. The agent selects actions based on the learned policy, and the video is rendered using the Gym environment.\n",
    "\n",
    "Finally, the code creates an instance of the CartPole environment, initializes the agent, and trains it for a specified number of episodes. After training, the agent's actions are recorded and saved as a video file.\n",
    "\n",
    "Note: To run this code, you will need to install the necessary dependencies, such as Gym, PyTorch, and TensorBoard.\n",
    "\n",
    "This code provides a practical implementation of an agent that can plan and act to achieve given objectives using noisy sensors and actuators. The agent leverages reinforcement learning techniques and is capable of recording a video of its actions after training."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

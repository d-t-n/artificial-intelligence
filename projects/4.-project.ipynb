{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project 4. for \"Evaluate the challenges in perception systems for AI,\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " I will provide you with a simplified Python code example that focuses on one specific challenge: object recognition using a state-of-the-art deep learning model called YOLO (You Only Look Once). This code demonstrates how to implement and evaluate an object detection system, addressing the challenge of real-time processing and accuracy.\n",
    "\n",
    "Please note that this is a simplified code snippet, and in a real-life scenario, you would need to consider additional steps, such as data collection, model training, and extensive testing. Also, you may need to adapt the code based on your specific environment and requirements. Here's an example:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load YOLO pre-trained weights and configuration files\n",
    "net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")\n",
    "\n",
    "# Define the classes that the YOLO model can detect\n",
    "classes = []\n",
    "with open(\"coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Set the input and output layers for the YOLO network\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "# Load the input image\n",
    "image = cv2.imread(\"test_image.jpg\")\n",
    "\n",
    "# Resize the image to the size required by the YOLO model\n",
    "height, width, channels = image.shape\n",
    "blob = cv2.dnn.blobFromImage(image, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "\n",
    "# Set the input blob for the network\n",
    "net.setInput(blob)\n",
    "outs = net.forward(output_layers)\n",
    "\n",
    "# Initialize lists to store detected objects' information\n",
    "class_ids = []\n",
    "confidences = []\n",
    "boxes = []\n",
    "\n",
    "# Process the outputs from YOLO to extract relevant information\n",
    "for out in outs:\n",
    "    for detection in out:\n",
    "        scores = detection[5:]\n",
    "        class_id = np.argmax(scores)\n",
    "        confidence = scores[class_id]\n",
    "        if confidence > 0.5:\n",
    "            # Object detected\n",
    "            center_x = int(detection[0] * width)\n",
    "            center_y = int(detection[1] * height)\n",
    "            w = int(detection[2] * width)\n",
    "            h = int(detection[3] * height)\n",
    "            \n",
    "            # Calculate the top-left corner coordinates of the bounding box\n",
    "            x = int(center_x - w / 2)\n",
    "            y = int(center_y - h / 2)\n",
    "            \n",
    "            # Store the detected object's information\n",
    "            class_ids.append(class_id)\n",
    "            confidences.append(float(confidence))\n",
    "            boxes.append([x, y, w, h])\n",
    "\n",
    "# Apply non-maximum suppression to remove redundant overlapping bounding boxes\n",
    "indices = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "\n",
    "# Draw the bounding boxes and labels on the image\n",
    "for i in indices:\n",
    "    i = i[0]\n",
    "    x, y, w, h = boxes[i]\n",
    "    label = str(classes[class_ids[i]])\n",
    "    confidence = confidences[i]\n",
    "    color = (255, 0, 0)  # Set the color for the bounding box\n",
    "    \n",
    "    # Draw the bounding box rectangle and label text\n",
    "    cv2.rectangle(image, (x, y), (x + w, y + h), color, 2)\n",
    "    cv2.putText(image, label + \" \" + str(round(confidence, 2)), (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "# Display the image with bounding boxes and labels\n",
    "cv2.imshow(\"Object Detection\", image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code snippet, the YOLO model is used for object detection. The weights and configuration files for the model need to be downloaded and provided (e.g., yolov3.weights and yolov3.cfg). Additionally, the coco.names file contains the names of the classes that the YOLO model can detect.\n",
    "\n",
    "The code loads an input image, resizes it to the size expected by the YOLO model, and passes it through the network. The outputs are then processed to extract the detected objects' information, and non-maximum suppression is applied to remove redundant bounding boxes. Finally, the code draws the bounding boxes and labels on the image and displays it.\n",
    "\n",
    "Please note that to run this code, you need to have OpenCV installed (pip install opencv-python). Additionally, you will need the YOLO model weights, configuration files, and the coco.names file, which you can obtain from the official YOLO website (https://pjreddie.com/darknet/yolo/).\n",
    "\n",
    "Remember that this is a simplified example focused on object recognition, and a comprehensive perception system would involve more steps and considerations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis:\n",
    "\n",
    "The evaluation of challenges in the perception system using YOLOv3 can be conducted by assessing its performance in terms of accuracy, speed, and robustness. Here are the key aspects to evaluate:\n",
    "\n",
    "Accuracy:\n",
    "Evaluate the accuracy of the object detection system by comparing the detected objects with ground truth annotations. Use evaluation metrics such as Intersection over Union (IoU) to measure the overlap between predicted and ground truth bounding boxes. Compute metrics like precision, recall, and F1 score to assess the system's ability to correctly detect objects and avoid false positives.\n",
    "\n",
    "Speed:\n",
    "Assess the processing speed of the perception system using YOLOv3. Measure the time taken for the system to process each frame or image and detect objects. Ensure that the system meets real-time processing requirements for the specific application. Compare the speed achieved by YOLOv3 against the desired target frame rate or processing time.\n",
    "\n",
    "Robustness:\n",
    "Evaluate the robustness of the system by testing it on diverse datasets and challenging scenarios. Include images or videos with varying lighting conditions, occlusions, scale variations, and cluttered backgrounds. Assess how well the system generalizes to different object categories and various instances of those objects. Measure the system's performance in both ideal and challenging conditions to identify potential limitations or biases.\n",
    "\n",
    "Handling Uncertainty:\n",
    "Analyze how well the system handles uncertainty and ambiguity in object detection. Evaluate its performance when faced with partially visible objects, low-resolution images, or situations where objects are heavily occluded. Assess the system's ability to provide accurate and reliable predictions under such circumstances.\n",
    "\n",
    "Edge Cases:\n",
    "Identify and analyze the performance of the perception system in edge cases, such as objects with unusual shapes or rare categories. Assess how well the system handles objects that are not adequately represented in the training data. Evaluate its ability to handle instances that deviate from the norm, considering the limitations and challenges associated with those cases.\n",
    "\n",
    "Quantitative and Qualitative Evaluation:\n",
    "Conduct both quantitative and qualitative evaluations of the perception system. Quantitative evaluation involves calculating metrics, such as accuracy, precision, recall, and F1 score, to provide measurable performance indicators. Qualitative evaluation includes visually inspecting the detection results, examining the bounding boxes and labels, and assessing their correctness.\n",
    "\n",
    "To ensure comprehensive evaluation, use appropriate benchmark datasets, such as COCO (Common Objects in Context), PASCAL VOC (Visual Object Classes), or custom datasets specific to your application domain. It is important to consider both the strengths and weaknesses of the perception system and iterate on the system design and training process based on the evaluation results.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

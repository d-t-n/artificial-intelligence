{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 5. Utilize sensors to execute perception tasks and their applications in intelligent systems\n",
    "## Project 5.1\n",
    "To provide a comprehensive real-life Python code example for utilizing sensors to execute perception tasks, I'll demonstrate a simple implementation using a camera sensor and OpenCV library for object detection. This code will detect objects in a live video stream from the camera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Load pre-trained model and configuration files\n",
    "net = cv2.dnn.readNetFromDarknet('yolov3.cfg', 'yolov3.weights')\n",
    "with open('coco.names', 'r') as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Set up camera capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Read frames from the camera\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Perform object detection\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1/255, (416, 416), swapRB=True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward()\n",
    "\n",
    "    # Process the detected objects\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > 0.5:\n",
    "                center_x = int(detection[0] * frame.shape[1])\n",
    "                center_y = int(detection[1] * frame.shape[0])\n",
    "                width = int(detection[2] * frame.shape[1])\n",
    "                height = int(detection[3] * frame.shape[0])\n",
    "                left = int(center_x - width / 2)\n",
    "                top = int(center_y - height / 2)\n",
    "                class_ids.append(class_id)\n",
    "                confidences.append(float(confidence))\n",
    "                boxes.append([left, top, width, height])\n",
    "\n",
    "    # Apply non-maximum suppression to remove redundant overlapping boxes\n",
    "    indices = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "\n",
    "    # Draw bounding boxes and labels on the frame\n",
    "    for i in indices:\n",
    "        i = i[0]\n",
    "        box = boxes[i]\n",
    "        left, top, width, height = box\n",
    "        label = f'{classes[class_ids[i]]}: {confidences[i]:.2f}'\n",
    "        cv2.rectangle(frame, (left, top), (left + width, top + height), (0, 255, 0), 2)\n",
    "        cv2.putText(frame, label, (left, top - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('Object Detection', frame)\n",
    "\n",
    "    # Exit loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code snippet, we utilize the camera sensor to capture a live video stream. The OpenCV library is used to perform object detection using the YOLOv3 model. The pre-trained model files (.cfg and .weights) need to be downloaded and provided. The coco.names file contains the names of the classes that the model can detect.\n",
    "\n",
    "The code sets up the camera capture and continuously reads frames from the camera. It then performs object detection on each frame using the YOLOv3 model. The detected objects are processed, and non-maximum suppression is applied to remove redundant bounding boxes. Finally, the code draws the bounding boxes and labels on the frame and displays it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project 5.2\n",
    "Here's another comprehensive real-life Python code example for utilizing sensors to execute perception tasks. This time, we'll focus on using a Lidar sensor for obstacle detection and mapping in a simulated robotic environment using the ROS (Robot Operating System) framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rospy\n",
    "from sensor_msgs.msg import LaserScan\n",
    "\n",
    "def lidar_callback(msg):\n",
    "    # Process Lidar data\n",
    "    ranges = msg.ranges\n",
    "    intensities = msg.intensities\n",
    "\n",
    "    # Perform obstacle detection and mapping\n",
    "    obstacles = []\n",
    "    for i in range(len(ranges)):\n",
    "        if ranges[i] < 2.0 and intensities[i] > 0.5:\n",
    "            obstacle = (ranges[i], i)\n",
    "            obstacles.append(obstacle)\n",
    "\n",
    "    # Print detected obstacles\n",
    "    for obstacle in obstacles:\n",
    "        distance, angle = obstacle\n",
    "        print(f\"Detected obstacle at distance: {distance:.2f} meters, angle: {angle} degrees\")\n",
    "\n",
    "def main():\n",
    "    # Initialize ROS node and subscribe to Lidar topic\n",
    "    rospy.init_node('lidar_obstacle_detection')\n",
    "    rospy.Subscriber('/laser_scan_topic', LaserScan, lidar_callback)\n",
    "\n",
    "    # Spin the node to receive messages\n",
    "    rospy.spin()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        main()\n",
    "    except rospy.ROSInterruptException:\n",
    "        pass\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code snippet, we use the ROS framework to interact with a Lidar sensor in a simulated robotic environment. The code subscribes to the /laser_scan_topic topic, which publishes the LaserScan messages containing the data from the Lidar sensor.\n",
    "\n",
    "The lidar_callback function is called whenever a new Lidar message is received. Inside the callback, we extract the ranges and intensities data from the message. We then process this data to perform obstacle detection and mapping. In this example, we consider objects as obstacles if they are within 2.0 meters and have an intensity greater than 0.5.\n",
    "\n",
    "The detected obstacles are stored in a list and printed to the console. However, in a real-world scenario, you might perform additional actions like obstacle avoidance, mapping, or path planning based on the detected obstacles.\n",
    "\n",
    "The main function initializes the ROS node, subscribes to the Lidar topic, and spins the node to receive messages continuously.\n",
    "\n",
    "Please note that this example assumes you have a ROS environment set up, including a simulated robotic environment with a Lidar sensor publishing data on the specified topic. Additionally, you may need to configure the code to match your specific setup, such as adjusting the topic name or obstacle detection criteria, depending on the specifications of your Lidar sensor and the requirements of your application."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume that you have already set up a simulated robotic environment with the ROS framework. To run this code, you will need to make the following adjustments:\n",
    "\n",
    "1 - Install ROS: Follow the instructions specific to your operating system from the official ROS website (http://www.ros.org/install).\n",
    "\n",
    "2 - Set Up a ROS Workspace: Create a new workspace or use an existing one using the following commands in the terminal:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $ mkdir -p ~/catkin_ws/src\n",
    "\n",
    "> $ cd ~/catkin_ws/\n",
    "\n",
    "> $ catkin_make"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 - Create a Package: Inside the src directory of your workspace, create a new package for your project using the following command:\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $ cd ~/catkin_ws/src\n",
    "\n",
    "> $ catkin_create_pkg lidar_obstacle_detection rospy sensor_msgs\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 - Download Lidar Data: Download and extract the sample lidar data for testing from the following link: Sample Lidar Data.\n",
    "\n",
    "5 - Modify the Code: Replace the /laser_scan_topic with the appropriate topic name for your Lidar sensor. Update the logic for obstacle detection and mapping according to your specific requirements.\n",
    "\n",
    "6 - Build and Run the Code: Build your workspace and run the code using the following commands:"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
